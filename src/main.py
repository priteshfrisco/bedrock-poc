#!/usr/bin/env python3
"""
UNIFIED ORCHESTRATOR - Local or AWS Cloud Processing
Supports both local file processing and S3/cloud processing
"""

import pandas as pd
import json
import sys
import os
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List
import traceback
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import openai

# Import pipeline components
from src.llm.gpt_client import GPTClient
from src.llm.prompt_builder import build_complete_prompt
from src.llm.tools import INGREDIENT_TOOL
from src.llm.tools.ingredient_lookup import lookup_ingredient
from src.llm.tools.health_focus_lookup import lookup_health_focus
from src.pipeline.utils.unit_converter import process_product_attributes
from src.pipeline.utils.business_rules import apply_all_business_rules
from src.pipeline.utils.high_level_category import assign_high_level_category
from src.core.preprocessing import is_non_supplement, standardize_dataframe
from src.core.log_manager import LogManager
from src.core.file_utils import write_csv
from src.core.file_tracker import FileTracker
from src.core.result_builder import build_error_result, build_success_result, build_filtered_result
from src.pipeline.step1_filter import generate_step1_audits, apply_step1_filter
from src.pipeline.step2_llm import extract_llm_attributes, extract_attributes_from_llm_result, extract_metadata_from_llm_result
from src.pipeline.step3_postprocess import apply_postprocessing

# AWS imports (only loaded in AWS mode)
try:
    import boto3
    from src.aws.s3_manager import S3Manager
    from src.aws.dynamodb_manager import DynamoDBManager
    AWS_AVAILABLE = True
except ImportError:
    AWS_AVAILABLE = False


def process_single_record(record: Dict, product_id: int, log_manager, max_retries: int = 3, test_step1_only: bool = False) -> Dict:
    """Process a single record through the complete pipeline - ORCHESTRATION ONLY"""
    start_time = datetime.now()
    asin = record.get('asin', f'P{product_id}')
    title = record.get('title', '')
    amazon_subcat = record.get('amazon_subcategory', '').lower().strip()
    
    result = {
        'product_id': product_id,
        'asin': asin,
        'title': title,
        'brand': record.get('brand', ''),
        'source_subcategory': amazon_subcat,
        'status': 'processing',
        'step_completed': 0
    }
    
    try:
        # ========== STEP 1: NON-SUPPLEMENT FILTERING ==========
        step1_result = apply_step1_filter(title, amazon_subcat, asin, log_manager)
        
        if not step1_result['passed']:
            # Filtered out
            result = build_filtered_result(
                result,
                step1_result['filter_reason'],
                step1_result['filter_type'],
                start_time,
                lookup_action=step1_result.get('action')
            )
            # üíæ Save Step 1 result immediately
            log_manager.save_audit_json('step1_filter', result, f"{asin}.json")
            return result
        
        # Passed filtering - store lookup action details
        result['lookup_action'] = step1_result['action']
        if step1_result['action'] == 'REMAP':
            result['nw_category'] = step1_result['nw_category']
            result['nw_subcategory'] = step1_result['nw_subcategory']
            result['remap_reason'] = step1_result['remap_reason']
        
        log_manager.log_step('step1_filter', f"[{asin}] PASSED filtering")
        
        # ‚ö†Ô∏è  TEST MODE: Stop after Step 1 if flag is set
        if test_step1_only:
            result['status'] = 'step1_complete'
            result['step_completed'] = 1
            result['processing_time_sec'] = (datetime.now() - start_time).total_seconds()
            # üíæ Save Step 1 result immediately
            log_manager.save_audit_json('step1_filter', result, f"{asin}.json")
            return result
        
        # ========== STEP 2: LLM EXTRACTION ==========
        llm_extraction_result = extract_llm_attributes(title, asin, product_id, log_manager, max_retries)
        
        if not llm_extraction_result['success']:
            result = build_error_result(result, llm_extraction_result['error'], 2, start_time)
            # üíæ Save Step 2 error immediately
            log_manager.save_audit_json('step2_llm', result, f"{asin}.json")
            return result
        
        llm_result = llm_extraction_result['data']
        
        # Extract attributes
        attributes = extract_attributes_from_llm_result(llm_result)
        age = attributes['age']
        gender = attributes['gender']
        form = attributes['form']
        organic = attributes['organic']
        
        # Process size/count/unit
        llm_result = process_product_attributes(llm_result)
        attributes = extract_attributes_from_llm_result(llm_result)
        count = attributes['count']
        unit = attributes['unit']
        size = attributes['size']
        ingredients = attributes['ingredients']
        business_rules = attributes.get('business_rules', {})
        
        # Extract metadata
        metadata = extract_metadata_from_llm_result(llm_result)
        
        # Build Step 2 result and save
        step2_result = result.copy()
        step2_result['status'] = 'step2_complete'
        step2_result['step_completed'] = 2
        step2_result['age'] = age
        step2_result['gender'] = gender
        step2_result['form'] = form
        step2_result['organic'] = organic
        step2_result['count'] = count
        step2_result['unit'] = unit
        step2_result['size'] = size
        step2_result['ingredients'] = ingredients
        step2_result['tokens_used'] = metadata['tokens_used']
        step2_result['api_cost'] = metadata['api_cost']
        step2_result['_metadata'] = metadata['_metadata']
        step2_result['processing_time_sec'] = (datetime.now() - start_time).total_seconds()
        # üíæ Save Step 2 result immediately
        log_manager.save_audit_json('step2_llm', step2_result, f"{asin}.json")
        
        # ========== STEP 3: POST-PROCESSING ==========
        # Business rules already applied by LLM, but still get health focus and high level category
        postprocess_result = apply_postprocessing(ingredients, age, gender, title, asin, log_manager)
        
        # Use business_rules from LLM if available, otherwise use postprocess_result
        if business_rules and business_rules.get('final_category'):
            category = business_rules.get('final_category')
            subcategory = business_rules.get('final_subcategory')
            primary_ingredient = business_rules.get('primary_ingredient')
            business_rules_reasoning = business_rules.get('reasoning', '')
            initial_category = business_rules.get('initial_category', category)
            initial_subcategory = business_rules.get('initial_subcategory', subcategory)
            has_changes = business_rules.get('has_changes', False)
            has_unknown = business_rules.get('has_unknown', False)
            
            # Log if changes were made
            if has_changes:
                log_manager.log_step(
                    'step3_postprocess',
                    f"[{asin}] Business rules changed: {initial_category}/{initial_subcategory} ‚Üí {category}/{subcategory}"
                )
            if has_unknown:
                log_manager.log_step('step3_postprocess', f"[{asin}] WARNING: Contains UNKNOWN values")
        else:
            # Fallback to Python business rules if LLM didn't call the tool
            category = postprocess_result['category']
            subcategory = postprocess_result['subcategory']
            primary_ingredient = postprocess_result['primary_ingredient']
            business_rules_reasoning = ''
            has_changes = False
            has_unknown = False
            log_manager.log_step('step3_postprocess', f"[{asin}] WARNING: LLM did not call business_rules tool, using Python fallback")
        
        # Build final result
        result['status'] = 'success'
        result['step_completed'] = 3
        result['age'] = age
        result['gender'] = gender
        result['form'] = form
        result['organic'] = organic
        result['count'] = count
        result['unit'] = unit
        result['size'] = size
        result['primary_ingredient'] = primary_ingredient
        result['num_ingredients'] = len(ingredients)
        
        # Store all ingredients (up to 20) like R system
        result['all_ingredients'] = []
        for idx, ing in enumerate(ingredients):
            ing_name = ing.get('name', '') if isinstance(ing, dict) else ing
            result['all_ingredients'].append(ing_name)
        
        result['category'] = category
        result['subcategory'] = subcategory
        result['health_focus'] = postprocess_result['health_focus']
        result['high_level_category'] = postprocess_result['high_level_category']
        
        # Add reasoning from business rules (only if changes were made or has unknown)
        result['business_rules_reasoning'] = business_rules_reasoning if (has_changes or has_unknown) else ''
        result['has_business_rule_changes'] = has_changes
        result['has_unknown_values'] = has_unknown
        
        result['tokens_used'] = metadata['tokens_used']
        result['api_cost'] = metadata['api_cost']
        result['_metadata'] = metadata['_metadata']
        result['processing_time_sec'] = (datetime.now() - start_time).total_seconds()
        
        # üíæ Save Step 3 (final) result immediately
        log_manager.save_audit_json('step3_postprocess', result, f"{asin}.json")
        
        log_manager.log_step('step3_postprocess', f"[{asin}] COMPLETE in {result['processing_time_sec']:.2f}s")
        
        return result
        
    except Exception as e:
        log_manager.log_step('error', f"[{asin}] EXCEPTION: {str(e)[:200]}")
        result['traceback'] = traceback.format_exc()
        error_result = build_error_result(result, str(e), result.get('step_completed', 0), start_time)
        # üíæ Save error immediately
        log_manager.save_audit_json('errors', error_result, f"{asin}.json")
        return error_result


def save_results(output_dir: Path, results: List[Dict], input_filename: str, log_manager: LogManager):
    """Save results to CSV only (R-system matching column names)"""
    log_manager.log_step('step4_output', f"Saving {len(results)} results to CSV...")
    
    # Save CSV with EXACT R system column names (from Master Item File)
    
    # Helper to get ingredient by index (1-indexed, starting from 2 for "other ing. 2")
    def get_ingredient(r, idx):
        """Get ingredient at index. idx=1 is primary, idx=2 is 'other ing. 2', etc."""
        all_ings = r.get('all_ingredients', [])
        if idx == 1 and r.get('primary_ingredient'):
            return r.get('primary_ingredient', '')
        elif idx <= len(all_ings):
            return all_ings[idx - 1]
        else:
            return ''
    
    df = pd.DataFrame([
        {
            # R System Core Columns (Business Data)
            'UPC': r.get('asin', ''),
            'Description': r['title'],
            'Brand': r['brand'],
            'NW Category': r.get('category', ''),
            'NW Subcategory': r.get('subcategory', ''),
            'NW Sub Brand 1': '',
            'NW Sub Brand 2': '',
            'NW Sub Brand 3': '',
            'Potency': '',
            'FORM': r.get('form', ''),
            'AGE': r.get('age', ''),
            'GENDER': r.get('gender', ''),
            'COMPANY': r['brand'],
            'FUNCTIONAL INGREDIENT': r.get('primary_ingredient', ''),
            'HEALTH FOCUS': r.get('health_focus', ''),
            'SIZE': r.get('size', ''),
            'HIGH LEVEL CATEGORY': r.get('high_level_category', ''),
            'NW_UPC': r.get('asin', ''),
            'Unit of Measure': r.get('unit', ''),
            'Pack Count': r.get('count', ''),
            'Organic': r.get('organic', ''),
            # Reasoning: Use business_rules_reasoning for supplements, filter_reason for filtered products
            'Reasoning': r.get('business_rules_reasoning', '') if r.get('status') == 'success' else r.get('filter_reason', ''),
            
            # Additional Ingredients (at the end)
            'other ing. 2': get_ingredient(r, 2),
            'other ing. 3': get_ingredient(r, 3),
            'other ing. 4': get_ingredient(r, 4),
            'other ing. 5': get_ingredient(r, 5),
            'other ing. 6': get_ingredient(r, 6),
            'other ing. 7': get_ingredient(r, 7),
            'other ing. 8': get_ingredient(r, 8),
            'other ing. 9': get_ingredient(r, 9),
            'other ing. 10': get_ingredient(r, 10),
            'other ing. 11': get_ingredient(r, 11),
            'other ing. 12': get_ingredient(r, 12),
            'other ing. 13': get_ingredient(r, 13),
            'other ing. 14': get_ingredient(r, 14),
            'other ing. 15': get_ingredient(r, 15),
            'other ing. 16': get_ingredient(r, 16),
            'other ing. 17': get_ingredient(r, 17),
            'other ing. 18': get_ingredient(r, 18),
            'other ing. 19': get_ingredient(r, 19),
            'other ing. 20': get_ingredient(r, 20),
            
            # Reserved for future use (at the end)
            'Spare1': '',
            'Spare2': '',
            'Spare3': '',
            'Spare4': '',
            'Spare5': ''
            
            # NOTE: Tracking columns (Product_ID, Status, Tokens, Cost, Time, Errors, etc.)
            # are stored in audit JSON files only, not in the CSV output
        }
        for r in results
    ])
    
    csv_file = output_dir / f"{input_filename}.csv"  # Just filename.csv
    df.to_csv(csv_file, index=False)
    
    log_manager.log_step('step4_output', f"Saved CSV: {csv_file} ({len(results)} records)")
    log_manager.log_step('step4_output', f"CSV has {len(df.columns)} columns")
    
    return csv_file


def save_audit_step_files(log_manager: LogManager, results: List[Dict], step: str):
    """Save per-product audit files for a specific step - Named by ASIN for easy tracking"""
    for result in results:
        asin = result.get('asin', f"P{result['product_id']:04d}")  # Fallback to product_id if no ASIN
        log_manager.save_audit_json(
            step_name=step,
            data=result,
            filename=f"{asin}.json"
        )


def main():
    print("="*80)
    print("PRODUCTION ORCHESTRATOR - PROCESS 1000+ RECORDS")
    print("="*80)
    
    start_time = datetime.now()
    print(f"\nStart Time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Configuration
    import sys
    INPUT_FILE = sys.argv[1] if len(sys.argv) > 1 else 'data/input/uncoded_100_records.csv'  # Allow command line override
    MAX_WORKERS = 100  # Number of parallel threads for maximum speed
    BATCH_SIZE = 100  # Save results every 100 records (intermediate saves for safety)
    
    # ‚ö†Ô∏è  TEST MODE: Stop after Step 1 (filtering only)
    TEST_STEP1_ONLY = False  # Run complete pipeline (Steps 1, 2, 3)  # Set to False to run full pipeline
    
    print(f"\nüìã Configuration:")
    print(f"   Input File: {INPUT_FILE}")
    print(f"   Max Workers: {MAX_WORKERS} (parallel API calls)")
    print(f"   Batch Size: {BATCH_SIZE} (save every N records)")
    
    # Load data
    print(f"\nüìÇ Loading data...")
    raw_df = pd.read_csv(INPUT_FILE, encoding='latin-1')
    print(f"‚úÖ Loaded {len(raw_df):,} raw records")
    
    # Standardize columns (rename, lowercase, clean)
    print(f"üîß Standardizing columns...")
    df = standardize_dataframe(raw_df)
    records = df.to_dict('records')
    total_records = len(records)
    
    print(f"‚úÖ Standardized {total_records:,} records")
    
    # Extract input filename (without path and extension)
    input_filename = Path(INPUT_FILE).stem  # e.g., "sample_10_test"
    
    # Initialize LogManager (handles all logging and audit structure)
    log_manager = LogManager(input_filename=input_filename, base_path='data')
    info = log_manager.get_info()
    
    # Initialize FileTracker (simple file-level status tracking)
    file_tracker = FileTracker()
    
    # Check if file is already being processed
    if not file_tracker.can_process(input_filename):
        print(f"‚ùå ERROR: File {input_filename} is already being processed!")
        return
    
    # Mark file as processing
    file_tracker.mark_processing(input_filename, info['run_id'], total_records)
    
    log_manager.log_step('run', "="*80)
    log_manager.log_step('run', f"STARTING PRODUCTION ORCHESTRATOR")
    log_manager.log_step('run', "="*80)
    log_manager.log_step('run', f"Input file: {INPUT_FILE}")
    log_manager.log_step('run', f"Total records: {total_records}")
    log_manager.log_step('run', f"Max workers: {MAX_WORKERS}")
    log_manager.log_step('run', f"Batch size: {BATCH_SIZE}")
    log_manager.log_step('run', f"File ID: {info['file_id']}")
    log_manager.log_step('run', f"Run ID: {info['run_id']}")
    
    # Initialize step-specific logs
    log_manager.log_step_start('step1_filter', 'STEP 1: NON-SUPPLEMENT FILTERING')
    log_manager.log_step_start('step2_llm', 'STEP 2: LLM EXTRACTION')
    log_manager.log_step_start('step3_postprocess', 'STEP 3: POST-PROCESSING (Business Rules, HF, HLC)')
    log_manager.log_step_start('step4_output', 'STEP 4: OUTPUT GENERATION')
    
    # Create output directory: data/output/{file_id}/run_{num}/
    output_dir = Path(f"data/output/{info['file_id']}/{info['run_id']}")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"üìÅ Output Directory: {output_dir}")
    print(f"üìÅ Logs Directory: {info['logs_path']}")
    print(f"üìÅ Audit Directory: {info['audit_path']}")
    print(f"üìÅ Tracking: data/tracking/{info['file_id']}_*.json")
    
    # Process records in parallel
    if TEST_STEP1_ONLY:
        print(f"\n‚ö†Ô∏è  TEST MODE: Running Step 1 (Filtering) ONLY")
        print(f"   LLM extraction will be SKIPPED")
    
    print(f"\nüöÄ Processing {total_records:,} records with {MAX_WORKERS} workers...")
    if not TEST_STEP1_ONLY:
        print(f"   Estimated time: ~{(total_records * 50) / MAX_WORKERS / 60:.0f} minutes")
    print()
    
    all_results = []
    batch_num = 0
    
    # Process in batches for progress saving
    for batch_start in range(0, total_records, BATCH_SIZE):
        batch_end = min(batch_start + BATCH_SIZE, total_records)
        batch_records = records[batch_start:batch_end]
        batch_num += 1
        
        print(f"üì¶ Batch {batch_num}: Records {batch_start+1}-{batch_end}")
        
        batch_results = []
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # Submit all tasks in this batch
            futures = {
                executor.submit(process_single_record, record, batch_start + idx + 1, log_manager, test_step1_only=TEST_STEP1_ONLY): idx 
                for idx, record in enumerate(batch_records)
            }
            
            # Collect results with progress bar
            for future in tqdm(as_completed(futures), total=len(futures), 
                             desc=f"  Processing", unit="record"):
                try:
                    result = future.result()
                    batch_results.append(result)
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  Task failed: {e}")
        
        # Sort batch results by product_id
        batch_results.sort(key=lambda x: x['product_id'])
        all_results.extend(batch_results)
        
        # Save intermediate results (only if full pipeline, not Step 1 test mode)
        if not TEST_STEP1_ONLY:
            csv_file = save_results(output_dir, all_results, input_filename, log_manager)
            # Save audit files for final step
            save_audit_step_files(log_manager, all_results, "final")
        
        log_manager.log_step('run', f"Batch {batch_num} complete: {len(batch_results)} products processed")
        
        # Print batch stats
        batch_success = sum(1 for r in batch_results if r['status'] == 'success')
        batch_step1_complete = sum(1 for r in batch_results if r['status'] == 'step1_complete')
        batch_filtered = sum(1 for r in batch_results if r['status'] == 'filtered_out')
        batch_errors = sum(1 for r in batch_results if r['status'] == 'error')
        
        if TEST_STEP1_ONLY:
            print(f"  ‚úÖ Passed Filter: {batch_step1_complete} | üö´ Filtered: {batch_filtered} | ‚ùå Errors: {batch_errors}")
        else:
            print(f"  ‚úÖ Success: {batch_success} | üö´ Filtered: {batch_filtered} | ‚ùå Errors: {batch_errors}")
        print(f"  üíæ Saved {len(all_results)} total results so far\n")
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    # Final statistics
    print("="*80)
    print("FINAL RESULTS")
    print("="*80)
    
    # Save Step 1 filtering audit files (AFTER all batches complete)
    generate_step1_audits(log_manager, all_results)
    
    success = [r for r in all_results if r['status'] == 'success']
    filtered = [r for r in all_results if r['status'] == 'filtered_out']
    step1_complete = [r for r in all_results if r['status'] == 'step1_complete']
    errors = [r for r in all_results if r['status'] == 'error']
    
    # Initialize cost and token variables
    total_cost = 0
    total_tokens = 0
    input_tokens = 0
    output_tokens = 0
    
    print(f"\nüìä OVERALL STATS:")
    print(f"   Total Records: {len(all_results):,}")
    
    if TEST_STEP1_ONLY:
        print(f"   ‚úÖ Step 1 Complete (Passed Filter): {len(step1_complete):,} ({len(step1_complete)/len(all_results)*100:.1f}%)")
        print(f"   üö´ Filtered Out: {len(filtered):,} ({len(filtered)/len(all_results)*100:.1f}%)")
        print(f"   ‚ùå Errors: {len(errors):,} ({len(errors)/len(all_results)*100:.1f}%)")
    else:
        print(f"   ‚úÖ Success: {len(success):,} ({len(success)/len(all_results)*100:.1f}%)")
        print(f"   üö´ Filtered: {len(filtered):,} ({len(filtered)/len(all_results)*100:.1f}%)")
        print(f"   ‚ùå Errors: {len(errors):,} ({len(errors)/len(all_results)*100:.1f}%)")
    
    if success:
        total_cost = sum(r['api_cost'] for r in success)
        total_tokens = sum(r['tokens_used'] for r in success)
        
        # Calculate input/output tokens separately
        input_tokens = 0
        output_tokens = 0
        for r in success:
            metadata = r.get('_metadata', {})
            tokens_breakdown = metadata.get('tokens', {})
            input_tokens += tokens_breakdown.get('prompt', 0)
            output_tokens += tokens_breakdown.get('completion', 0)
        
        avg_time = sum(r['processing_time_sec'] for r in success) / len(success)
        total_sequential = sum(r['processing_time_sec'] for r in success)
        
        print(f"\nüí∞ COST:")
        print(f"   Total API Cost: ${total_cost:.4f}")
        print(f"   Avg per product: ${total_cost/len(success):.6f}")
        print(f"   Total tokens: {total_tokens:,}")
        print(f"   Input tokens: {input_tokens:,}")
        print(f"   Output tokens: {output_tokens:,}")
        
        print(f"\n‚è±Ô∏è  TIMING:")
        print(f"   Total (parallel): {duration:.2f}s ({duration/60:.2f} min)")
        print(f"   Avg per product: {avg_time:.2f}s")
        print(f"   Sequential would be: {total_sequential/60:.2f} min")
        print(f"   ‚ú® Speedup: {total_sequential / duration:.1f}x")
        print(f"   Throughput: {len(all_results) / (duration/60):.1f} records/min")
    
    # Category breakdown
    if success:
        categories = {}
        hlcs = {}
        for r in success:
            cat = r.get('category', 'UNKNOWN')
            hlc = r.get('high_level_category', 'UNKNOWN')
            categories[cat] = categories.get(cat, 0) + 1
            hlcs[hlc] = hlcs.get(hlc, 0) + 1
        
        print(f"\nüì¶ TOP 10 CATEGORIES:")
        for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"   {cat}: {count}")
        
        print(f"\nüéØ HIGH LEVEL CATEGORIES:")
        for hlc, count in sorted(hlcs.items(), key=lambda x: x[1], reverse=True):
            print(f"   {hlc}: {count}")
    
    print(f"\nüìÅ OUTPUT FILES:")
    if TEST_STEP1_ONLY:
        print(f"   Step 1 Audit: {info['audit_path']}/step1_filter/")
        print(f"   Logs: {info['logs_path']}/")
        print(f"   (CSV and Final audit not generated in Step 1 test mode)")
    else:
        print(f"   CSV: {csv_file}")
        print(f"   Audit: {info['audit_path']}/final/ ({len(all_results)} files)")
        print(f"   Logs: {info['logs_path']}/")
    
    log_manager.log_step('run', f"="*80)
    log_manager.log_step('run', f"PROCESSING COMPLETE")
    log_manager.log_step('run', f"="*80)
    log_manager.log_step('run', f"Success: {len(success)}, Filtered: {len(filtered)}, Errors: {len(errors)}")
    if success:
        log_manager.log_step('run', f"Total cost: ${total_cost:.4f}")
        log_manager.log_step('run', f"Total tokens: {total_tokens:,} (input: {input_tokens:,}, output: {output_tokens:,})")
    log_manager.log_step('run', f"Total duration: {duration:.2f}s")
    if not TEST_STEP1_ONLY:
        log_manager.log_step('run', f"Output CSV: {csv_file}")
    
    # Save run manifest
    manifest_data = {
        'total_records': len(all_results),
        'success': len(success),
        'filtered': len(filtered),
        'errors': len(errors),
        'total_cost': total_cost if success else 0,
        'total_tokens': total_tokens if success else 0,
        'input_tokens': input_tokens if success else 0,
        'output_tokens': output_tokens if success else 0,
        'duration_seconds': duration,
        'test_mode_step1_only': TEST_STEP1_ONLY
    }
    if not TEST_STEP1_ONLY:
        manifest_data['output_csv'] = str(csv_file)
    log_manager.save_run_manifest(manifest_data)
    
    # Mark file as completed in tracker
    file_tracker.mark_completed(
        filename=input_filename,
        run_id=info['run_id'],
        success=len(success),
        filtered=len(filtered),
        errors=len(errors),
        total_cost=total_cost if success else 0,
        total_tokens=total_tokens if success else 0,
        input_tokens=input_tokens if success else 0,
        output_tokens=output_tokens if success else 0,
        duration_seconds=duration
    )
    
    print(f"\n" + "="*80)
    print(f"End Time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Total Duration: {duration:.2f}s ({duration/60:.2f} minutes)")
    print("="*80)
    print("\n‚úÖ PRODUCTION RUN COMPLETE!")


def send_notification(sns_topic_arn: str, subject: str, message: str):
    """Send SNS notification (AWS mode only)"""
    try:
        sns = boto3.client('sns')
        sns.publish(
            TopicArn=sns_topic_arn,
            Subject=subject,
            Message=message
        )
        print(f"üìß Notification sent: {subject}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to send notification: {str(e)}")


def process_aws_mode(
    input_bucket: str,
    input_key: str,
    output_bucket: str,
    audit_bucket: str,
    dynamodb_table: str,
    run_id: str,
    sns_topic_arn: str = None
):
    """
    AWS Cloud Processing Mode
    Reads from S3, processes, writes to S3, tracks in DynamoDB
    """
    if not AWS_AVAILABLE:
        print("‚ùå AWS mode requires boto3 and AWS modules")
        sys.exit(1)
    
    start_time = datetime.now()
    
    print("="*80)
    print("AWS CLOUD PROCESSING - Bedrock AI Data Enrichment")
    print("="*80)
    print(f"\nRun ID: {run_id}")
    print(f"Input: s3://{input_bucket}/{input_key}")
    print(f"Output: s3://{output_bucket}/")
    print(f"Audit: s3://{audit_bucket}/")
    
    try:
        # Initialize AWS managers
        s3 = S3Manager()
        db = DynamoDBManager(dynamodb_table)
        
        # Read input CSV from S3
        print(f"\nüì• Reading input data...")
        df = s3.read_csv_from_s3(input_bucket, input_key, encoding='latin-1')
        
        if df is None:
            print("‚ùå Failed to read input file")
            return
        
        # Standardize
        df = standardize_dataframe(df)
        total_records = len(df)
        print(f"‚úÖ Loaded {total_records:,} records")
        
        # Create log manager (will write to local /tmp then upload to S3)
        input_filename = Path(input_key).stem
        log_manager = LogManager(
            input_filename=input_filename,
            base_path='/tmp/bedrock-data'
        )
        
        print(f"\nüöÄ Processing {total_records} records...")
        
        results = []
        
        for idx, record in df.iterrows():
            product_id = idx + 1
            asin = record.get('asin', f'P{product_id}')
            title = record.get('title', '')
            
            try:
                # Step 1: Filter
                step1_result = apply_step1_filter(
                    title,
                    record.get('amazon_subcategory', '').lower().strip(),
                    asin,
                    log_manager
                )
                
                if not step1_result['passed']:
                    # Filtered - write to DynamoDB
                    db.put_record(
                        asin=asin,
                        run_id=run_id,
                        status='filtered',
                        data={'reason': step1_result['filter_reason']}
                    )
                    continue
                
                # Step 2: LLM extraction
                llm_result = extract_llm_attributes(title, asin, product_id, log_manager)
                
                if not llm_result['success']:
                    # Error
                    db.put_record(
                        asin=asin,
                        run_id=run_id,
                        status='error',
                        data={'error': llm_result.get('error', 'Unknown error')}
                    )
                    continue
                
                # Extract attributes
                attrs = extract_attributes_from_llm_result(llm_result['data'])
                business_rules_result = attrs.get('business_rules', {})
                
                # Build result
                result = {
                    'asin': asin,
                    'title': title,
                    'brand': record.get('brand', ''),
                    'category': business_rules_result.get('category', 'UNKNOWN'),
                    'subcategory': business_rules_result.get('subcategory', 'UNKNOWN'),
                    'primary_ingredient': business_rules_result.get('primary_ingredient', ''),
                    'age': attrs['age'],
                    'gender': attrs['gender'],
                    'form': attrs['form'],
                    'organic': attrs.get('organic', ''),
                    'count': attrs.get('count', ''),
                    'unit': attrs.get('unit', ''),
                    'size': attrs.get('size', ''),
                    'health_focus': business_rules_result.get('health_focus', ''),
                    'high_level_category': business_rules_result.get('high_level_category', ''),
                    'reasoning': business_rules_result.get('reasoning', '')
                }
                
                results.append(result)
                
                # Write to DynamoDB
                db.put_record(
                    asin=asin,
                    run_id=run_id,
                    status='success',
                    data=result
                )
                
                if product_id % 100 == 0:
                    print(f"‚úÖ [{product_id}/{total_records}] Processed")
                
            except Exception as e:
                print(f"‚ùå [{product_id}/{total_records}] {asin}: {str(e)}")
                db.put_record(
                    asin=asin,
                    run_id=run_id,
                    status='error',
                    data={'error': str(e)}
                )
        
        # Convert results to DataFrame
        if results:
            results_df = pd.DataFrame(results)
            
            # Write to S3
            output_key = f"runs/{run_id}/{input_filename}_coded.csv"
            s3.write_csv_to_s3(results_df, output_bucket, output_key)
            
            print(f"\n‚úÖ Processing complete!")
            print(f"   Processed: {len(results)} products")
            print(f"   Output: s3://{output_bucket}/{output_key}")
        
        # Upload audit files to S3
        audit_prefix = f"runs/{run_id}/audit"
        audit_dir = Path(f'/tmp/bedrock-data/audit/{input_filename}')
        if audit_dir.exists():
            count = s3.upload_directory(audit_dir, audit_bucket, audit_prefix)
            print(f"   Uploaded {count} audit files to S3")
        
        # Upload logs to S3 (same structure as local)
        logs_prefix = f"runs/{run_id}/logs"
        logs_dir = Path(f'/tmp/bedrock-data/logs/{input_filename}')
        if logs_dir.exists():
            count = s3.upload_directory(logs_dir, audit_bucket, logs_prefix)
            print(f"   Uploaded {count} log files to S3")
        
        # Send success notification
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds() / 60
        
        if sns_topic_arn:
            message = f"""
‚úÖ Processing Complete!

File: {input_key}
Run ID: {run_id}
Total Products: {total_records:,}
Processed: {len(results):,}
Duration: {duration:.1f} minutes

Output: s3://{output_bucket}/runs/{run_id}/{input_filename}_coded.csv
Audit: s3://{audit_bucket}/runs/{run_id}/audit/
Logs: s3://{audit_bucket}/runs/{run_id}/logs/

Status Summary:
- Success: {len(results)}
- Total: {total_records}
"""
            send_notification(sns_topic_arn, f"‚úÖ Processing Complete - {input_filename}", message)
    
    except Exception as e:
        # Send error notification
        if sns_topic_arn:
            error_message = f"""
‚ùå Processing Failed!

File: {input_key}
Run ID: {run_id}
Error: {str(e)}

Please check CloudWatch Logs for details.
"""
            send_notification(sns_topic_arn, f"‚ùå Processing Failed - {Path(input_key).stem}", error_message)
        raise


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Bedrock AI Data Enrichment Pipeline')
    parser.add_argument('--mode', choices=['local', 'aws'], default='local',
                       help='Execution mode: local (default) or aws (cloud)')
    
    # AWS mode arguments
    parser.add_argument('--input-bucket', help='S3 input bucket (AWS mode)')
    parser.add_argument('--input-key', help='S3 input key (AWS mode)')
    parser.add_argument('--output-bucket', help='S3 output bucket (AWS mode)')
    parser.add_argument('--audit-bucket', help='S3 audit bucket (AWS mode)')
    parser.add_argument('--dynamodb-table', help='DynamoDB table name (AWS mode)')
    parser.add_argument('--run-id', help='Run ID (AWS mode)')
    parser.add_argument('--sns-topic-arn', help='SNS topic ARN for notifications (AWS mode)')
    
    args = parser.parse_args()
    
    if args.mode == 'aws':
        # AWS mode - use env vars as fallback
        INPUT_BUCKET = args.input_bucket or os.getenv('INPUT_BUCKET', 'bedrock-ai-data-enrichment-input-081671069810')
        INPUT_KEY = args.input_key or os.getenv('INPUT_KEY', 'sample_10_test.csv')
        OUTPUT_BUCKET = args.output_bucket or os.getenv('OUTPUT_BUCKET', 'bedrock-ai-data-enrichment-output-081671069810')
        AUDIT_BUCKET = args.audit_bucket or os.getenv('AUDIT_BUCKET', 'bedrock-ai-data-enrichment-audit-081671069810')
        DYNAMODB_TABLE = args.dynamodb_table or os.getenv('DYNAMODB_TABLE', 'bedrock-ai-data-enrichment-processing-state')
        RUN_ID = args.run_id or os.getenv('RUN_ID', f"run-{datetime.now().strftime('%Y%m%d-%H%M%S')}")
        SNS_TOPIC_ARN = args.sns_topic_arn or os.getenv('SNS_TOPIC_ARN')
        
        process_aws_mode(
            input_bucket=INPUT_BUCKET,
            input_key=INPUT_KEY,
            output_bucket=OUTPUT_BUCKET,
            audit_bucket=AUDIT_BUCKET,
            dynamodb_table=DYNAMODB_TABLE,
            run_id=RUN_ID,
            sns_topic_arn=SNS_TOPIC_ARN
        )
    else:
        # Local mode - existing behavior
        main()

