# Bedrock AI Data Enrichment - Deployment Package

## Complete Production Setup (Single Command Deploy)

This is a **production-ready** deployment package that can be transferred to any client environment.

---

## ğŸ“¦ What's Included

### Infrastructure (Terraform)
- **Single `main.tf` file** - Complete infrastructure as code
- S3 buckets (input/output/audit/reference)
- DynamoDB state tracking
- ECS Fargate cluster
- Lambda trigger
- SNS notifications
- CloudWatch Logs
- All IAM roles/permissions

### Application Code
- Python pipeline with GPT-5-mini
- AWS integration (S3, DynamoDB, SNS)
- Docker containerization
- Reference data (ingredient lookups, business rules)

### Deployment Scripts
- `deploy.sh` - One command to deploy everything
- `upload_reference_data.sh` - Upload CSVs/JSONs to S3
- `deploy_docker.sh` - Build and push Docker image
- `test_aws_setup.sh` - Verify AWS connectivity

---

## ğŸš€ One-Command Deployment

```bash
# Set your configuration
export TF_VAR_notification_email="your-email@company.com"
export TF_VAR_openai_api_key="sk-your-openai-key"
export AWS_PROFILE=your-aws-profile

# Deploy everything (takes ~15 minutes)
cd infrastructure
./deploy.sh
```

**That's it!** The script will:
1. âœ… Deploy Terraform infrastructure
2. âœ… Upload reference data to S3
3. âœ… Build and push Docker image to ECR
4. âœ… Test the setup
5. âœ… Show you next steps

---

## ğŸ“‹ Prerequisites

1. **AWS Account** with permissions to create:
   - S3 buckets
   - DynamoDB tables
   - ECS clusters
   - Lambda functions
   - SNS topics
   - IAM roles

2. **Tools Installed:**
   - AWS CLI (`aws --version`)
   - Terraform (`terraform --version`)
   - Docker (`docker --version`)
   - Python 3.13+ (`python --version`)

3. **Credentials:**
   - AWS credentials configured (`aws configure`)
   - OpenAI API key
   - Email for notifications

---

## ğŸ—ï¸ Architecture

```
Upload CSV â†’ S3 Input Bucket
    â†“
S3 Event â†’ Lambda Trigger
    â†“
Lambda â†’ Starts ECS Fargate Task
    â†“
ECS Task â†’ Processes 60K products (2-3 hours)
    â†“
ECS Task â†’ Writes results to S3 Output Bucket
    â†“
ECS Task â†’ Sends SNS email notification
    â†“
ECS Task â†’ Auto-terminates
```

---

## ğŸ“Š Cost Estimate

**Per 60K product run:**
- OpenAI API: ~$60
- AWS (ECS + S3 + DynamoDB): ~$2
- **Total: ~$62 per run**

For 2 files/month: **~$124/month**

---

## ğŸ“ File Structure

```
bedrock-poc/
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ terraform/
â”‚   â”‚   â””â”€â”€ main.tf              # Single Terraform file
â”‚   â”œâ”€â”€ deploy.sh                # One-command deployment
â”‚   â”œâ”€â”€ deploy_docker.sh         # Docker build/push
â”‚   â”œâ”€â”€ upload_reference_data.sh # Upload CSVs/JSONs
â”‚   â””â”€â”€ test_aws_setup.sh        # Verify setup
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main_aws.py              # Cloud entry point
â”‚   â”œâ”€â”€ main.py                  # Local entry point
â”‚   â”œâ”€â”€ aws/                     # AWS integrations
â”‚   â”œâ”€â”€ core/                    # Core logic
â”‚   â”œâ”€â”€ llm/                     # LLM prompts/tools
â”‚   â””â”€â”€ pipeline/                # Processing steps
â”œâ”€â”€ reference_data/              # Lookups & rules
â”œâ”€â”€ Dockerfile                   # Container definition
â””â”€â”€ requirements.txt             # Python dependencies
```

---

## ğŸ”§ Configuration

All configuration in Terraform variables:

```hcl
# infrastructure/terraform/main.tf
variable "notification_email" {
  default = "your-email@company.com"
}

variable "openai_api_key" {
  default = "sk-your-key"
  sensitive = true
}

variable "aws_region" {
  default = "us-east-2"
}
```

Override with environment variables or `terraform.tfvars`:
```bash
export TF_VAR_notification_email="your-email"
export TF_VAR_openai_api_key="sk-key"
```

---

## ğŸ§ª Testing

After deployment:

```bash
# Upload test file
aws s3 cp data/input/sample_10_test.csv s3://[input-bucket]/

# Watch logs in real-time
aws logs tail /ecs/bedrock-ai-data-enrichment-task --follow

# Check email for completion notification

# Download results
aws s3 cp s3://[output-bucket]/runs/[run-id]/sample_10_test_coded.csv ./
```

---

## ğŸ“§ Email Notifications

You'll receive emails for:
- âœ… **Success**: Processing complete with summary
- âŒ **Failure**: Error details with CloudWatch link

Example email:
```
âœ… Processing Complete!

File: uncoded_60k_products.csv
Total Products: 60,000
Processed: 58,234
Duration: 142.3 minutes

Output: s3://[bucket]/runs/[run-id]/uncoded_60k_products_coded.csv
```

---

## ğŸ—‘ï¸ Cleanup

To remove all resources:

```bash
cd infrastructure/terraform
terraform destroy
```

---

## ğŸ“– Detailed Documentation

- `docs/COMPLETE_DEPLOYMENT.md` - Step-by-step deployment guide
- `docs/AWS_DEPLOYMENT.md` - AWS architecture details
- `docs/AWS_CHECKLIST.md` - Prerequisites checklist
- `docs/DOCKER_SETUP.md` - Docker deployment options

---

## ğŸ¤ Client Handoff

**This package is ready to transfer to client:**

1. âœ… Single Terraform file (no fragmentation)
2. âœ… One-command deployment script
3. âœ… All configuration via environment variables
4. âœ… Professional documentation
5. âœ… Testing scripts included
6. âœ… Cost estimates provided
7. âœ… Cleanup instructions

**Transfer:** Just ZIP this entire folder and send to client.

---

## ğŸ†˜ Support

For issues:
1. Check CloudWatch Logs: `/ecs/bedrock-ai-data-enrichment-task`
2. Check DynamoDB table for per-product status
3. Review `docs/` folder for troubleshooting guides

---

## ğŸ“ License & Credits

- Built with OpenAI GPT-5-mini
- AWS infrastructure managed by Terraform
- Python 3.13+ application

